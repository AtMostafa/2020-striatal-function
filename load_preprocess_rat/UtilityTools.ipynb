{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isClock(sig,fs):\n",
    "    sigM=np.mean(sig)\n",
    "    sig2=sig-sigM\n",
    "    sig2=np.sign(sig2)#clock is a perfect binary signal now\n",
    "    if len(sig) > 5*60*fs: #5min\n",
    "        L=int(5*60*fs)\n",
    "    else:\n",
    "        L=len(sig)-1\n",
    "    if np.corrcoef(sig[:L],sig2[:L])[0,1] > 0.98:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def prm_reader(prmFile):\n",
    "    CWD=os.getcwd()\n",
    "    try:\n",
    "        os.chdir(os.path.dirname(prmFile))\n",
    "        prmName=os.path.basename(prmFile)\n",
    "        %run $prmName\n",
    "    finally:\n",
    "        os.chdir(CWD)\n",
    "    return globals()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading entire excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadExcelFile:\n",
    "    def __init__(self,root,animal,fileName=None):\n",
    "        self.animal=animal\n",
    "        self.excelPath=''\n",
    "        if fileName is None:\n",
    "            path=os.path.join(root,animal,animal)+'*.xls*'\n",
    "            excelfiles=glob.glob(path)\n",
    "            assert len(excelfiles)!=0, \"No Excel files\"+path\n",
    "            assert len(excelfiles) ==1, \"Too many Excel files\"+str(excelfiles)\n",
    "            self.excelPath=excelfiles[0]\n",
    "        else:\n",
    "            self.excelPath=fileName\n",
    "            assert os.path.isfile(self.excelPath), \"Bad Excel file path\"\n",
    "        \n",
    "        self.read_excel_file()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \" \".join(['Excel file at:',self.excelPath])\n",
    "\n",
    "    \n",
    "    def read_excel_file(self):\n",
    "        safeExcel=safe_copy_from_nas(self.excelPath)\n",
    "        path=safeExcel.start()\n",
    "        with pd.ExcelFile(path) as file:\n",
    "            sheets=file.sheet_names\n",
    "            self.excelData={sheet:pd.read_excel(file,sheet) for sheet in sheets}\n",
    "        safeExcel.stop(fileTypes=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find files based on extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file(path, extension=['.raw.kwd']):\n",
    "    \"\"\"\n",
    "    This function finds all the file types specified by 'extension' (ex: *.dat) in the 'path' directory\n",
    "    and all its subdirectories and their sub-subdirectories etc., \n",
    "    and returns a list of all file paths\n",
    "    'extension' is a list of desired file extensions: ['.dat','.prm']\n",
    "    \"\"\"\n",
    "    if type(extension) is str:\n",
    "        extension=extension.split()   #turning extension into a list with a single element\n",
    "    return [os.path.join(walking[0],goodfile) for walking in list(os.walk(path)) \n",
    "         for goodfile in walking[2] for ext in extension if goodfile.endswith(ext)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safe working with NAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class safe_copy_from_nas:\n",
    "    \n",
    "    def __init__(self,path):\n",
    "        assert isinstance(path,str)\n",
    "        self.filePath=path\n",
    "    \n",
    "    def start(self,tempName='.NASqwrytdvn2u1r'):\n",
    "        \"\"\"\n",
    "        safely copying the file to a temp local dir\n",
    "        \"\"\"\n",
    "        if \"NAS\" in self.filePath or \"NETDATA\" in self.filePath:\n",
    "            logging.info(\"Copying from NAS... \"+self.filePath)\n",
    "            defaultPath=os.path.expanduser(\"~\") #home folder\n",
    "            self.tempdir=os.path.join(defaultPath,tempName)\n",
    "            try:\n",
    "                os.mkdir(self.tempdir)\n",
    "            except FileExistsError: #in case of multiple files being copied the directory already exists\n",
    "                pass\n",
    "            try:\n",
    "                self.newPath=copy(self.filePath,self.tempdir)\n",
    "            except Exception as e:\n",
    "                logging.warning(\"could not copy from NAS to local drive!\"+self.filePath)\n",
    "                logging.info(repr(e))\n",
    "                self.newPath=\"\"\n",
    "        else:\n",
    "            self.newPath=self.filePath\n",
    "        return self.newPath\n",
    "    \n",
    "    def stop(self,fileTypes=['.prm','.dat','.eeg']):\n",
    "        \"\"\"\n",
    "        uploading all files with fileTypes found in the temp directory back to\n",
    "        the remote server, and removing everything from local drive\n",
    "        \"\"\"\n",
    "        if \"NAS\" in self.filePath or \"NETDATA\" in self.filePath:\n",
    "            files=find_file(self.tempdir,fileTypes)             \n",
    "            try:    \n",
    "                for newfile in files:\n",
    "                    copy(newfile,os.path.dirname(self.filePath))\n",
    "                    logging.info(\"Uploaded to NAS! \"+newfile)\n",
    "            except Exception as e:  #Exceptions should not be raised in this level!\n",
    "                logging.warning(\"upload to NAS failed! \"+newfile)\n",
    "                logging.info(repr(e))\n",
    "            finally:\n",
    "                #remove everything, ignore errors\n",
    "                rmtree(self.tempdir,ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class permtest_output:\n",
    "    def __init__(self,D0,shuffledD=None,p_val=None,band=None, pairwise_CI=None,sig_signal=None):\n",
    "        self.statistic=D0\n",
    "        self.shuffled_data=shuffledD\n",
    "        self.p_val=p_val\n",
    "        self.significant=sig_signal\n",
    "        self.boundary=band\n",
    "        self.pairwise_alpha=pairwise_CI\n",
    "        self.significant=sig_signal\n",
    "\n",
    "        \n",
    "def perm_statistic(x,y,Nx,Ny,sigma=0):\n",
    "    \n",
    "    if len(x) <2:\n",
    "        return (x/Nx)-(y/Ny)\n",
    "    \n",
    "    #the gaussian kernel\n",
    "    x_smooth=scipy.ndimage.filters.gaussian_filter1d(x, sigma=sigma, order=0, mode='constant', cval=0, truncate=4.0)\n",
    "    y_smooth=scipy.ndimage.filters.gaussian_filter1d(y, sigma=sigma, order=0, mode='constant', cval=0, truncate=4.0)\n",
    "    \n",
    "    return (x_smooth/Nx)-(y_smooth/Ny)\n",
    "    \n",
    "def permtest(x,y, iterN=1000,sigma=0.05):\n",
    "    \"\"\"\n",
    "    Permutation test as to whether x>y or not.\n",
    "    x,y:\n",
    "    represent the data. they could be eitherr one dimentional(several realizations)\n",
    "    or 2-D (several realizaions through out the time/space/... course)\n",
    "        EX: x.shape==(15,500) means 15 trials/samples over 500 time bins\n",
    "    \n",
    "    iterN:\n",
    "    number of iterations used to shuffle. max(iterN)=(len(x)+len(y))!/len(x)!len(y)!\n",
    "    \n",
    "    sigma:\n",
    "    the standard deviation of the gaussian kernel used for smoothing when there are multiple data points,\n",
    "    based on the Fujisawa 2008 paper, default value: 0.05\n",
    "    \"\"\"\n",
    "    \n",
    "    #input check\n",
    "    if x.ndim>2 or y.ndim>2:\n",
    "        raise ValueError('bad input dimentions')\n",
    "    elif x.ndim==1 or y.ndim==1:\n",
    "        x=np.reshape(x,(len(x),1))\n",
    "        y=np.reshape(y,(len(y),1))\n",
    "    \n",
    "    #computing the tset statistic\n",
    "    xTrial,yTrial=x.shape[0],y.shape[0]\n",
    "    \n",
    "    x_superimpos=np.nansum(x,axis=0)\n",
    "    y_superimpos=np.nansum(y,axis=0)\n",
    "    \n",
    "    D0=perm_statistic(x_superimpos,y_superimpos,x.shape[0],y.shape[0])\n",
    "    \n",
    "    # shuffling the data\n",
    "    Dshuffled=np.ones((iterN,len(x_superimpos)))*np.nan\n",
    "    for i in range(iterN):\n",
    "        tmpShuffle=np.concatenate((x,y),axis=0)\n",
    "        np.random.shuffle(tmpShuffle)  #works in-plcae\n",
    "        xNew,yNew=tmpShuffle[:xTrial,:],tmpShuffle[xTrial:,:]\n",
    "        \n",
    "        xNew_superimpos=np.nansum(xNew,axis=0)\n",
    "        yNew_superimpos=np.nansum(yNew,axis=0)\n",
    "        \n",
    "        Dshuffled[i,:]=perm_statistic(xNew_superimpos,yNew_superimpos,xNew.shape[0],yNew.shape[0],sigma)\n",
    "    \n",
    "    if len(D0)<2:  #single point comparison\n",
    "        p_val0=np.sum(Dshuffled>=D0,axis=0)/(iterN+1)\n",
    "        return permtest_output(D0=D0,p_val=p_val0,shuffledD=Dshuffled,sig_signal=bool(p_val0<=0.05))\n",
    "    \n",
    "    #global bands\n",
    "    alpha=100\n",
    "    CI=5  #global confidance interval\n",
    "    pairwise_high_band=np.percentile(a=Dshuffled,q=100-CI,axis=0)\n",
    "    \n",
    "    while alpha>=5:\n",
    "        high_band=np.percentile(a=Dshuffled,q=100-CI,axis=0)\n",
    "        breaks=np.sum([np.sum(Dshuffled[i,:]>high_band)>1 for i in range(iterN)])\n",
    "        alpha=(breaks/iterN)*100\n",
    "        CI=0.95*CI\n",
    "        logging.info(\"Global Confidence interval at \"+CI+'\\nComputing again...\\n')\n",
    "    \n",
    "    #finding significant bins\n",
    "    global_sig=D0>high_band\n",
    "    pairwise_sig=D0>pairwise_high_band\n",
    "    sigIndex=np.where(global_sig)[0]\n",
    "    \n",
    "    for i in sigIndex:\n",
    "        if i==0 or i==len(global_sig):\n",
    "            continue\n",
    "        global_sig[np.min((np.where(pairwise_sig[:i])[0][-1],i)):np.max((np.where(pairwise_sig[i:])[0][-1],i))]=True\n",
    "    \n",
    "    return permtest_output(D0=D0,shuffledD=Dshuffled,sig_signal=global_sig,p_val=CI,band=high_band)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample size control by random selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample_size_control:\n",
    "    def __init__(self,func,animalList,NbAnimal,n,**kwargs):\n",
    "        \"\"\"\n",
    "        func: function to be applied to the randomly chosen animals\n",
    "        NbAnimal: number of animals to be considered in each iteration of func\n",
    "        n: max number of iterations\n",
    "        kwargs: give any input necessary to run \"func\" comma-seperated, like normal function arguments\n",
    "        \"\"\"\n",
    "        if NbAnimal>len(animalList):\n",
    "            raise (\"NbAnimal must be smaller than animal list\")\n",
    "#         if n==0:\n",
    "#             tmp=scipy.special.comb(len(animalList), NbAnimal, exact=True, repetition=False)\n",
    "#             n=max([1000,tmp])\n",
    "        \n",
    "        self.iterN=n\n",
    "        self.animalList=animalList\n",
    "        self.func=func\n",
    "        self.subsetSize=NbAnimal\n",
    "        self.kwargs=kwargs\n",
    "        \n",
    "        self.animalRepeat=np.ones(len(self.animalList))\n",
    "\n",
    "        self.Results=self.run_function()\n",
    "        \n",
    "        \n",
    "    def random_animal_subset(self):\n",
    "        prob=np.sum(self.animalRepeat)*(1/self.animalRepeat)\n",
    "        prob=prob/np.sum(prob)\n",
    "        animalListSubset=np.random.choice(a=self.animalList,size=self.subsetSize,replace=False,p=prob)\n",
    "        self.animalRepeat+=[animal in animalListSubset for animal in self.animalList]\n",
    "\n",
    "        return animalListSubset\n",
    "            \n",
    "    def run_function(self):\n",
    "        #inputArgs=inspect.getargspec(self.func)[0]\n",
    "        i=0\n",
    "        result=[]\n",
    "        Args=self.kwargs\n",
    "\n",
    "        #calculating estimated time needed to process\n",
    "        t0=time.perf_counter()\n",
    "        Args.update({'animalList':self.random_animal_subset()})\n",
    "        result.append(self.func(**Args))\n",
    "        Args.update({'animalList':self.random_animal_subset()})\n",
    "        result.append(self.func(**Args))\n",
    "        i=2\n",
    "        t_elapsed=(time.perf_counter()-t0)/2\n",
    "        logging.info(\"Estimated time to run sample size control<\"+str(t_elapsed*self.iterN)+'s')\n",
    "\n",
    "        while i<self.iterN:\n",
    "            Args.update({'animalList':self.random_animal_subset()})\n",
    "            result.append(self.func(**Args))\n",
    "            i+=1\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### session RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rmse(data,onlyGood=False,maxTreadmillLength=90,raw=False):\n",
    "    '''\n",
    "    Compute the rmse of the position between trial start and trial stop (treadmill stop)\n",
    "    plots and returns the RMSE matrix\n",
    "    '''\n",
    "    allTraj=get_positions_array_beginning(data,onlyGood=onlyGood,raw=raw)\n",
    "    NbTrial=allTraj.shape[0]\n",
    "    \n",
    "    if NbTrial<3:\n",
    "        title=\"Not enough trials\"\n",
    "        med=np.nan\n",
    "        return med\n",
    "\n",
    "    rmse=np.ones((NbTrial,NbTrial))*(-1)\n",
    "\n",
    "    for i in range(NbTrial):\n",
    "        rmse[i,i]=0\n",
    "        for j in range(i+1,NbTrial):\n",
    "            maxL=min([np.sum(np.logical_not(np.isnan(allTraj[i,:]))),np.sum(np.logical_not(np.isnan(allTraj[j,:])))])\n",
    "            rmse[i,j]=np.sqrt(np.sum((allTraj[i,:maxL]-allTraj[j,:maxL])**2)/maxL)\n",
    "    \n",
    "    RMSEmatrix=np.triu(rmse,k=0)+np.triu(rmse,k=0).T #symetrical\n",
    "    RMSEmatrix/=maxTreadmillLength\n",
    "    pp=plt.pcolor(RMSEmatrix,vmin=0,vmax=1,cmap=\"Reds\")\n",
    "    plt.colorbar(pp)\n",
    "    plt.xlim([0,RMSEmatrix.shape[0]])\n",
    "    plt.ylim([0,RMSEmatrix.shape[1]])\n",
    "    \n",
    "    #median of upper triangle of matrix\n",
    "    coef=RMSEmatrix[np.tril_indices(RMSEmatrix.shape[0],-1)]\n",
    "    #print(len(coef))\n",
    "    med=np.nanmedian(coef)\n",
    "    maxSecond=allTraj.shape[1]/float(data.cameraSamplingRate)\n",
    "    #title of the plot\n",
    "    title=\"\"\n",
    "    if onlyGood:\n",
    "        title=\"Good Trials\"\n",
    "    else:\n",
    "        title=\"All Trials\"\n",
    "\n",
    "    title+=', trajectory median  r= %.2f'%med     \n",
    "    plt.title(title)\n",
    "\n",
    "    \n",
    "    return med,RMSEmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory_PDF(data,TimeRes=.5,PosRes=5,onlyGood=False,**kargs):\n",
    "    \"\"\"\n",
    "    calculates and plots the joint PDF of trajectories.\n",
    "    time resolution in seconds\n",
    "    Position resolution in cm\n",
    "    \"\"\"\n",
    "    \n",
    "    #data=Data(root,session[:6],session,defaultParam,redoPreprocess=False)\n",
    "    allTraj=get_positions_array_beginning(data,onlyGood).T\n",
    "    trialDuration=scipy.stats.mode(data.maxTrialDuration)[0]\n",
    "\n",
    "    posSize =len(np.arange(data.treadmillRange[0],data.treadmillRange[1],PosRes))\n",
    "    timeSize=len(np.arange(0,trialDuration,TimeRes))\n",
    "    trajDis=np.zeros([timeSize,posSize])\n",
    "    \n",
    "    #replacing nans w/ the last position\n",
    "    allTraj=allTraj//PosRes\n",
    "\n",
    "    for t in range(allTraj.shape[0]-1):\n",
    "        timeIndex=int((t/data.cameraSamplingRate)//TimeRes)\n",
    "        trajDis[timeIndex,:]=[np.sum(allTraj[t,:]==x) for x in range(posSize)]\n",
    "\n",
    "    trajDis=scipy.ndimage.filters.gaussian_filter(trajDis, sigma=[1,1],\n",
    "                                                  order=0, mode='nearest', truncate=3)\n",
    "    #normalizing as a PDF\n",
    "    trajDis/=np.sum(trajDis)\n",
    "\n",
    "#     plt.figure();\n",
    "    plt.pcolor(trajDis.T, cmap=cm.hot,**kargs);\n",
    "    ax=plt.gca();\n",
    "    ax.set_xticks     (np.linspace(0,timeSize,5));\n",
    "    ax.set_xticklabels(np.linspace(0,trialDuration,5));\n",
    "    ax.set_yticks     (np.linspace(0,posSize,10));\n",
    "    ax.set_yticklabels(np.linspace(data.treadmillRange[0],data.treadmillRange[1],10));\n",
    "    \n",
    "    return trajDis\n",
    "\n",
    "def twoD_entropy(trajDist):\n",
    "    H=0\n",
    "    for i in range(trajDist.shape[0]):\n",
    "        for j in range(trajDist.shape[1]):\n",
    "            try:\n",
    "                H+=trajDist[i,j]*math.log(float(trajDist[i,j]),2)\n",
    "            except:\n",
    "                pass\n",
    "    H=-H\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read session files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(data,paramName,extension=\".behav_param\",exclude=None,valueType=str):\n",
    "    '''\n",
    "    Use to read from .behav_param or .entrancetimes\n",
    "    Look for lines containing \"paramName\" and not containing \"exclude\"\n",
    "    Split them by white spaces \n",
    "    example: \"treadmill speed:     30.00\" becomes [\"treadmill\",\"speed:\",\"30.00\"])\n",
    "    Return a list of their last element, in the specified valueType (in example: \"30.00\")\n",
    "    '''\n",
    "    behav=data.fullPath+extension\n",
    "    if not os.path.exists(behav):\n",
    "        print(\"No file %s\"%behav)\n",
    "        data.hasBehavior=False\n",
    "        return []\n",
    "    result=[]\n",
    "    trials=[0]\n",
    "    with open(behav,\"r\") as f:\n",
    "        for line in f:\n",
    "            if \"Trial #\" in line:\n",
    "                trials.append(int(float(line.split()[-1]))-1)\n",
    "            if paramName in line:\n",
    "                if (exclude is not None) and (exclude in line):\n",
    "                    continue\n",
    "                res=line.split()[-1]\n",
    "                #integer or float: replace comma by dots\n",
    "                if valueType in [int,float]:\n",
    "                    res=res.replace(\",\",\".\")                 \n",
    "                #integer: convert first to float (\"0.00\" -> 0.00 -> 0)\n",
    "                if valueType is int:\n",
    "                    res=int(float(res))\n",
    "                #boolean \"TRUE\" \"FALSE\"\n",
    "                elif valueType is bool:\n",
    "                    res=(res.lower()==\"true\")\n",
    "                else:\n",
    "                    res=valueType(res)\n",
    "                result.append( (trials[-1],res) )\n",
    "    out=[np.nan]*(trials[-1]+1)\n",
    "    for item in result:\n",
    "        out[item[0]]=item[1]\n",
    "    return np.asarray(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_rate(timePoints,minDis=0,maxDis=np.inf):\n",
    "    \"\"\"\n",
    "    timePoints: list of times of occuring of events (in sec)\n",
    "    minDis= minimum distance between events to be considered valid\n",
    "    \"\"\"\n",
    "    tDiff=np.diff(timePoints)\n",
    "    tDiff=tDiff[np.logical_and(tDiff>minDis,tDiff<maxDis)]\n",
    "    return 1/np.nanmean(tDiff)\n",
    "\n",
    "def compute_rate (x,winLen,overlap=0.5,zero=0,end=None):\n",
    "    \"\"\"\n",
    "    x: list like data with times of event, in sec\n",
    "    winLen: length of window in sec\n",
    "    overlap: normalized overlap: (0,1)\n",
    "    zero: begining of the time axis\n",
    "    end: maximum of time axis\n",
    "    ??window: window param of scipy.signal.get_window\n",
    "    \"\"\"\n",
    "    assert overlap<1 and overlap>0, \"bad overlap value\"\n",
    "    x=np.array(x)\n",
    "    if end is None:\n",
    "        end=x[-1]\n",
    "#     if window is None:\n",
    "#         window='boxcar'\n",
    "#     win=scipy.signal.get_window(window,winLen)\n",
    "    Range=np.arange(zero,end,(1-overlap)*winLen)\n",
    "    out=[]\n",
    "    for i,_ in enumerate(Range):\n",
    "        a=x[np.logical_and(x>=Range[i],x<Range[i]+winLen)]\n",
    "        out.append(len(a)/winLen)\n",
    "    return np.array(out),Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy PRB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prb_copy (prbfile, animalFolder):\n",
    "    \"\"\"\n",
    "    prbfile='/home/david/Mostafa/info/prb-config files/8tetrode_8channelgroup.prb'\n",
    "    animalFolder='/NETDATA/Rat172/Experiments/'\n",
    "    \n",
    "    \"\"\"\n",
    "    for dat in find_file(animalFolder, extension=['.dat']):\n",
    "        prb2=copy(prbfile,os.path.dirname(dat))\n",
    "        os.rename(prb2,dat[:-3]+'prb')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a single channel from a _*.dat_ file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ephy_epoch(filename, fs, Nch, wantedCh, t0, t1):\n",
    "    assert filename.endswith(('.dat','.DAT','.Dat')), \"bad file type: Not *.DAT\"\n",
    "    \n",
    "    sampleSize=np.dtype(np.int16).itemsize\n",
    "    systembyte=sys.byteorder\n",
    "    n0=int(t0*fs*Nch*sampleSize)\n",
    "    \n",
    "    signal=[]\n",
    "\n",
    "    with open(filename,'rb') as f:\n",
    "        if t1>t0:\n",
    "            n1=int(t1*fs*Nch*sampleSize)\n",
    "        elif t1==-1:\n",
    "            f.seek(0,2)\n",
    "            n1=f.tell()\n",
    "        else:\n",
    "            raise ValueError(\"t1 must be greater than t0, or -1\")\n",
    "\n",
    "        f.seek( n0 + ((wantedCh-1) *sampleSize))\n",
    "        n=n0\n",
    "        step=(Nch-1)*sampleSize\n",
    "        while n < n1:\n",
    "            data=f.read(sampleSize)\n",
    "            signal.append(int.from_bytes(data,systembyte,signed=True))\n",
    "            n+=step+sampleSize\n",
    "            f.seek(step,1)\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "    return np.array(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging several .dat files together \n",
    "(MUST have the same number of channels and sampling frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dat_merger(files: list, nCh: int):\n",
    "    \"\"\"\n",
    "    files: a list of all the dat file paths you wish to merge (a list of strings)\n",
    "    nCh: number of channels (int)\n",
    "    \"\"\"\n",
    "    dat=[]\n",
    "    for file in files:\n",
    "        data=np.fromfile(file)\n",
    "        data=np.reshape(data,(-1,nCh))\n",
    "        dat.append(data)\n",
    "\n",
    "    out=np.concatenate([array for array in dat],axis=0)\n",
    "    del dat\n",
    "    path=f'{os.path.dirname(files[0])}{os.sep}MERGED.dat'\n",
    "    out.tofile(path)\n",
    "    print(f'saved in {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_fetch(root: str, animal: str, profile: dict, PerfParam: list, NbSession: slice =5):\n",
    "    \"\"\"\n",
    "    returns the data requested by PerfParam\n",
    "    PerfParam: a list of known performance parameters or functions recieving data objest as input\n",
    "    \"\"\"\n",
    "    if not isinstance(PerfParam,list):\n",
    "        PerfParam=[PerfParam]\n",
    "    \n",
    "    perf=[]\n",
    "    func=[]\n",
    "    for item in PerfParam:\n",
    "        if isinstance(item,types.FunctionType):\n",
    "            func.append(item)\n",
    "        elif isinstance(item,str):\n",
    "            perf.append(item)\n",
    "\n",
    "    if not isinstance(NbSession,slice):\n",
    "        if NbSession >0:\n",
    "            NbSession=slice(NbSession)\n",
    "        else:\n",
    "            NbSession=slice(NbSession,None)\n",
    "\n",
    "    sessions=batch_get_session_list(root,[animal],profile=profile)['Sessions'][NbSession]\n",
    "#     assert < len(sessions), \"not enough sessions with this profile\"\n",
    "    \n",
    "    res=dict((param,[]) for param in perf)\n",
    "    res.update((param.__name__,[]) for param in func)\n",
    "    for session in sessions:\n",
    "        data=Data(root,session[:6],session,redoPreprocess=False)\n",
    "        \n",
    "        p1=compute_or_read_stats(data, perf, \n",
    "                                 saveAsPickle=False, redo=False)            \n",
    "        for param in perf:\n",
    "            res[param].append(p1[param])\n",
    "            \n",
    "        for fun in func:\n",
    "            res[fun.__name__].append(fun(data))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Times when trials end (Treadmill stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "def punishment_duration(data,trial,minDuration=1,maxDuration=10):\n",
    "    beamIgnore=read_file(data,paramName=\"consider beam state after (s)\",valueType=float)[trial]\n",
    "    punishTime=maxDuration-maxDuration*(data.entranceTime[trial]-beamIgnore)/(data.goalTime[trial]-beamIgnore)\n",
    "    punishTime=max((punishTime,minDuration))\n",
    "    return punishTime\n",
    "\n",
    "def detect_trial_end(data, trials=None):\n",
    "    if trials is None:\n",
    "        trials=data.trials\n",
    "        \n",
    "    for trial in trials:\n",
    "        if data.timeEndTrial[trial] is not None:\n",
    "            if trial in data.goodTrials or data.timeEndTrial[trial] >= data.entranceTime[trial]+.99:\n",
    "                continue\n",
    "        if data.entranceTime[trial] > data.goalTime[trial]:\n",
    "            data.timeEndTrial[trial]=data.entranceTime[trial]\n",
    "            data.indexEndTrial[trial]=int_(data.timeEndTrial[trial]*data.cameraSamplingRate)\n",
    "            continue\n",
    "        else: #implementing the punishment rule\n",
    "            data.timeEndTrial[trial]=data.entranceTime[trial]+punishment_duration(data,trial)\n",
    "            data.indexEndTrial[trial]=int_(data.timeEndTrial[trial]*data.cameraSamplingRate)\n",
    "            \n",
    "    return data.timeEndTrial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Ordered colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colors(n, colormap='plasma'):\n",
    "    colors = []\n",
    "    cmap = plt.cm.get_cmap(colormap)\n",
    "    for colorVal in np.linspace(0, 1, n):\n",
    "        colors.append(cmap(colorVal))\n",
    "    return colors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
