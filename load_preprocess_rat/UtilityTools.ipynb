{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isClock(sig,fs):\n",
    "    sigM=np.mean(sig)\n",
    "    sig2=sig-sigM\n",
    "    sig2=np.sign(sig2)#clock is a perfect binary signal now\n",
    "    if len(sig) > 5*60*fs: #5min\n",
    "        L=int(5*60*fs)\n",
    "    else:\n",
    "        L=len(sig)-1\n",
    "    if np.corrcoef(sig[:L],sig2[:L])[0,1] > 0.98:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def prm_reader(prmFile):\n",
    "    CWD=os.getcwd()\n",
    "    try:\n",
    "        os.chdir(os.path.dirname(prmFile))\n",
    "        prmName=os.path.basename(prmFile)\n",
    "        %run $prmName\n",
    "    finally:\n",
    "        os.chdir(CWD)\n",
    "    return globals()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading entire excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadExcelFile:\n",
    "    def __init__(self,root,animal,fileName=None):\n",
    "        self.animal=animal\n",
    "        self.excelPath=''\n",
    "        if fileName is None:\n",
    "            path=os.path.join(root,animal,animal)+'*.xls*'\n",
    "            excelfiles=glob.glob(path)\n",
    "            assert len(excelfiles)!=0, \"No Excel files\"+path\n",
    "            assert len(excelfiles) ==1, \"Too many Excel files\"+str(excelfiles)\n",
    "            self.excelPath=excelfiles[0]\n",
    "        else:\n",
    "            self.excelPath=fileName\n",
    "            assert os.path.isfile(self.excelPath), \"Bad Excel file path\"\n",
    "        \n",
    "        self.read_excel_file()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \" \".join(['Excel file at:',self.excelPath])\n",
    "\n",
    "    \n",
    "    def read_excel_file(self):\n",
    "        safeExcel=safe_copy_from_nas(self.excelPath)\n",
    "        path=safeExcel.start()\n",
    "        with pd.ExcelFile(path) as file:\n",
    "            sheets=file.sheet_names\n",
    "            self.excelData={sheet:pd.read_excel(file,sheet) for sheet in sheets}\n",
    "        safeExcel.stop(fileTypes=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find files based on extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file(path, extension=['.raw.kwd']):\n",
    "    \"\"\"\n",
    "    This function finds all the file types specified by 'extension' (ex: *.dat) in the 'path' directory\n",
    "    and all its subdirectories and their sub-subdirectories etc., \n",
    "    and returns a list of all file paths\n",
    "    'extension' is a list of desired file extensions: ['.dat','.prm']\n",
    "    \"\"\"\n",
    "    if type(extension) is str:\n",
    "        extension=extension.split()   #turning extension into a list with a single element\n",
    "    return [os.path.join(walking[0],goodfile) for walking in list(os.walk(path)) \n",
    "         for goodfile in walking[2] for ext in extension if goodfile.endswith(ext)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safe working with NAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class safe_copy_from_nas:\n",
    "    \n",
    "    def __init__(self,path):\n",
    "        assert isinstance(path,str)\n",
    "        self.filePath=path\n",
    "    \n",
    "    def start(self,tempName='.NASqwrytdvn2u1r'):\n",
    "        \"\"\"\n",
    "        safely copying the file to a temp local dir\n",
    "        \"\"\"\n",
    "        if \"NAS\" in self.filePath or \"NETDATA\" in self.filePath:\n",
    "            logging.info(\"Copying from NAS... \"+self.filePath)\n",
    "            defaultPath=os.path.expanduser(\"~\") #home folder\n",
    "            self.tempdir=os.path.join(defaultPath,tempName)\n",
    "            try:\n",
    "                os.mkdir(self.tempdir)\n",
    "            except FileExistsError: #in case of multiple files being copied the directory already exists\n",
    "                pass\n",
    "            try:\n",
    "                self.newPath=copy(self.filePath,self.tempdir)\n",
    "            except Exception as e:\n",
    "                logging.warning(\"could not copy from NAS to local drive!\"+self.filePath)\n",
    "                logging.info(repr(e))\n",
    "                self.newPath=\"\"\n",
    "        else:\n",
    "            self.newPath=self.filePath\n",
    "        return self.newPath\n",
    "    \n",
    "    def stop(self,fileTypes=['.prm','.dat','.eeg']):\n",
    "        \"\"\"\n",
    "        uploading all files with fileTypes found in the temp directory back to\n",
    "        the remote server, and removing everything from local drive\n",
    "        \"\"\"\n",
    "        if \"NAS\" in self.filePath or \"NETDATA\" in self.filePath:\n",
    "            files=find_file(self.tempdir,fileTypes)             \n",
    "            try:    \n",
    "                for newfile in files:\n",
    "                    copy(newfile,os.path.dirname(self.filePath))\n",
    "                    logging.info(\"Uploaded to NAS! \"+newfile)\n",
    "            except Exception as e:  #Exceptions should not be raised in this level!\n",
    "                logging.warning(\"upload to NAS failed! \"+newfile)\n",
    "                logging.info(repr(e))\n",
    "            finally:\n",
    "                #remove everything, ignore errors\n",
    "                rmtree(self.tempdir,ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class permtest:\n",
    "    \"\"\"\n",
    "    Permutation test as to whether there is significant difference between group one and two.\n",
    "    \n",
    "    group1, group2: Represent the data. they could be either one dimentional (several realizations)\n",
    "        or 2-D (several realizaions through out the time/space/... course)\n",
    "        EX: x.shape==(15,500) means 15 trials/samples over 500 time bins\n",
    "\n",
    "    nIterations: Number of iterations used to shuffle. max(iterN)=(len(x)+len(y))!/len(x)!len(y)!\n",
    "\n",
    "    initGlobConfInterval:\n",
    "        Initial value for the global confidence band.\n",
    "\n",
    "    smoothSigma: the standard deviation of the gaussian kernel used for smoothing when there are multiple data points,\n",
    "        based on the Fujisawa 2008 paper, default value: 0.05\n",
    "\n",
    "    Outputs:\n",
    "        pVal: P-values\n",
    "        highBand, lowBand: AKA boundary. Represents global bands.\n",
    "        significantDiff: An array of True or False, indicating whether there is a difference.\n",
    "    \n",
    "    \"\"\"  \n",
    "    def __init__(self, group1, group2, nIterations=1000, initGlobConfInterval=5, smoothSigma=0.05, randomSeed=1):\n",
    "        self.__group1, self.__group2 = self.__setGroupData(group1), self.__setGroupData(group2)\n",
    "        self.__nIterations, self.__smoothSigma = nIterations, smoothSigma\n",
    "        self.__initGlobConfInterval = initGlobConfInterval\n",
    "        self.__randomSeed = randomSeed\n",
    "        \n",
    "        self.__checkGroups()\n",
    "\n",
    "        # origGroupDiff is also known as D0 in the definition of permutation test.\n",
    "        self.__origGroupDiff = self.__computeGroupDiff(group1, group2)\n",
    "\n",
    "        # Generate surrogate groups, compute difference of mean for each group, and put in a matrix.\n",
    "        self.__diffSurGroups = self.__setDiffSurrGroups()\n",
    "\n",
    "        # Set statistics\n",
    "        self.pVal = self.__setPVal()\n",
    "        self.highBand, self.lowBand = self.__setBands()\n",
    "        self.pairwiseHighBand = self.__setPairwiseHighBand()\n",
    "        self.pairwiseLowBand = self.__setPairwiseLowBand()\n",
    "        self.significantDiff = self.__setSignificantGroup()\n",
    "\n",
    "    def __setGroupData(self, groupData):\n",
    "        if not isinstance(groupData, dict):\n",
    "            return groupData\n",
    "\n",
    "        realizations = list(groupData.values())\n",
    "        subgroups = list(groupData.keys())\n",
    "                    \n",
    "        dataMat = np.zeros((len(subgroups), len(realizations[0])))\n",
    "        for index, realization in enumerate(realizations):\n",
    "            if len(realization) != len(realizations[0]):\n",
    "                raise Exception(\"The length of all realizations in the group dictionary must be the same\")\n",
    "            \n",
    "            dataMat[index] = realization\n",
    "\n",
    "        return dataMat\n",
    "\n",
    "    def __checkGroups(self):\n",
    "        # input check\n",
    "        if not isinstance(self.__group1, np.ndarray) or not isinstance(self.__group2, np.ndarray):\n",
    "            raise ValueError(\"In permutation test, \\\"group1\\\" and \\\"group2\\\" should be numpy arrays.\")\n",
    "\n",
    "        if self.__group1.ndim > 2 or self.__group2.ndim > 2:\n",
    "            raise ValueError('In permutation test, the groups must be either vectors or matrices.')\n",
    "\n",
    "        elif self.__group1.ndim == 1 or self.__group2.ndim == 1:\n",
    "            self.__group1 = np.reshape(self.__group1, (len(self.__group1), 1))\n",
    "            self.__group2 = np.reshape(self.__group2, (len(self.__group2), 1))\n",
    "\n",
    "    def __computeGroupDiff(self, group1, group2):\n",
    "        meanDiff = np.nanmean(group1, axis=0) - np.nanmean(group2, axis=0)\n",
    "        \n",
    "        if len(self.__group1[0]) == 1 and len(self.__group2[0]) == 1:\n",
    "            return meanDiff\n",
    "        \n",
    "        return smooth(meanDiff, sigma=self.__smoothSigma)\n",
    "\n",
    "    def __setDiffSurrGroups(self):\n",
    "        # Fix seed \n",
    "        np.random.seed(seed=self.__randomSeed)\n",
    "        # shuffling the data\n",
    "        self.__concatenatedData = np.concatenate((self.__group1,  self.__group2), axis=0)\n",
    "        \n",
    "        diffSurrGroups = np.zeros((self.__nIterations, self.__group1.shape[1]))\n",
    "        for iteration in range(self.__nIterations):\n",
    "            # Generate surrogate groups\n",
    "            # Shuffle every column.\n",
    "            np.random.shuffle(self.__concatenatedData)  \n",
    "\n",
    "            # Return surrogate groups of same size.            \n",
    "            surrGroup1, surrGroup2 = self.__concatenatedData[:self.__group1.shape[0], :], self.__concatenatedData[self.__group1.shape[0]:, :]\n",
    "            \n",
    "            # Compute the difference between mean of surrogate groups\n",
    "            surrGroupDiff = self.__computeGroupDiff(surrGroup1, surrGroup2)\n",
    "            \n",
    "            # Store individual differences in a matrix.\n",
    "            diffSurrGroups[iteration, :] = surrGroupDiff\n",
    "\n",
    "        return diffSurrGroups\n",
    " \n",
    "    def __setPVal(self):\n",
    "        positivePVals = np.sum(1*(self.__diffSurGroups > self.__origGroupDiff), axis=0) / self.__nIterations\n",
    "        negativePVals = np.sum(1*(self.__diffSurGroups < self.__origGroupDiff), axis=0) / self.__nIterations\n",
    "        return np.array([np.min([1, 2*pPos, 2*pNeg]) for pPos, pNeg in zip(positivePVals, negativePVals)])\n",
    "\n",
    "    def __setBands(self):\n",
    "        if not isinstance(self.__origGroupDiff, np.ndarray):  # single point comparison\n",
    "            return None, None\n",
    "        \n",
    "        alpha = 100 # Global alpha value\n",
    "        highGlobCI = self.__initGlobConfInterval  # global confidance interval\n",
    "        lowGlobCI = self.__initGlobConfInterval  # global confidance interval\n",
    "        while alpha >= 5:\n",
    "            # highBand = np.percentile(a=self.__diffSurGroups, q=100-highGlobCI, axis=0)\n",
    "            # lowBand = np.percentile(a=self.__diffSurGroups, q=lowGlobCI, axis=0)\n",
    "\n",
    "            highBand = np.percentile(a=self.__diffSurGroups, q=100-highGlobCI)\n",
    "            lowBand = np.percentile(a=self.__diffSurGroups, q=lowGlobCI)\n",
    "\n",
    "            breaksPositive = np.sum(\n",
    "                [np.sum(self.__diffSurGroups[i, :] > highBand) > 1 for i in range(self.__nIterations)]) \n",
    "            \n",
    "            breaksNegative = np.sum(\n",
    "                [np.sum(self.__diffSurGroups[i, :] < lowBand) > 1 for i in range(self.__nIterations)])\n",
    "            \n",
    "            alpha = ((breaksPositive + breaksNegative) / self.__nIterations) * 100\n",
    "            highGlobCI = 0.95 * highGlobCI\n",
    "            lowGlobCI = 0.95 * lowGlobCI\n",
    "        return highBand, lowBand           \n",
    "\n",
    "    def __setSignificantGroup(self):\n",
    "        if not isinstance(self.__origGroupDiff, np.ndarray):  # single point comparison\n",
    "            return self.pVal <= 0.05\n",
    "\n",
    "        # finding significant bins\n",
    "        globalSig = np.logical_or(self.__origGroupDiff > self.highBand, self.__origGroupDiff < self.lowBand)\n",
    "        pairwiseSig = np.logical_or(self.__origGroupDiff > self.__setPairwiseHighBand(), self.__origGroupDiff < self.__setPairwiseLowBand())\n",
    "        \n",
    "        significantGroup = globalSig.copy()\n",
    "        lastIndex = 0\n",
    "        for currentIndex in range(len(pairwiseSig)):\n",
    "            if (globalSig[currentIndex] == True):\n",
    "                lastIndex = self.__setNeighborsToTrue(significantGroup, pairwiseSig, currentIndex, lastIndex)\n",
    "\n",
    "        return significantGroup\n",
    "    \n",
    "    def __setPairwiseHighBand(self, localBandValue=0.5):        \n",
    "        return np.percentile(a=self.__diffSurGroups, q=100 - localBandValue, axis=0)\n",
    "\n",
    "    def __setPairwiseLowBand(self, localBandValue=0.5):        \n",
    "        return np.percentile(a=self.__diffSurGroups, q=localBandValue, axis=0)\n",
    "\n",
    "    def __setNeighborsToTrue(self, significantGroup, pairwiseSig, currentIndex, previousIndex):\n",
    "        \"\"\"\n",
    "            While the neighbors of a global point pass the local band (consecutively), set the global band to true.\n",
    "            Returns the last index which was set to True.\n",
    "        \"\"\" \n",
    "        if (currentIndex < previousIndex):\n",
    "            return previousIndex\n",
    "        \n",
    "        for index in range(currentIndex, previousIndex, -1):\n",
    "            if (pairwiseSig[index] == True):\n",
    "                significantGroup[index] = True\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        previousIndex = currentIndex\n",
    "        for index in range(currentIndex + 1, len(significantGroup)):\n",
    "            previousIndex = index\n",
    "            if (pairwiseSig[index] == True):\n",
    "                significantGroup[index] = True\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return previousIndex\n",
    "    \n",
    "    def plotSignificant(self,ax: plt.Axes.axes,y: float,x=None,**kwargs):\n",
    "        if x is None:\n",
    "            x=np.arange(0,len(self.significantDiff))+1\n",
    "        for x0,x1,p in zip(x[:-1],x[1:],self.significantDiff):\n",
    "            if p:\n",
    "                ax.plot([x0,x1],[y,y],zorder=-2,**kwargs)\n",
    "                \n",
    "    @staticmethod\n",
    "    def plotSigPair(ax: plt.Axes.axes,y: float,x=None, s: str ='*',**kwargs):\n",
    "        if x is None:\n",
    "            x=(0,len(self.significantDiff))\n",
    "        if 'color' not in kwargs:\n",
    "            kwargs['color']='k'\n",
    "        \n",
    "        dy=.03*(ax.get_ylim()[1]-ax.get_ylim()[0])\n",
    "        ax.plot(x,[y,y],**kwargs)\n",
    "        ax.plot([x[0],x[0]],[y-dy,y],[x[1],x[1]],[y-dy,y],**kwargs)\n",
    "        ax.text(np.mean(x),y,s=s,\n",
    "                ha='center',va='center',color=kwargs['color'],\n",
    "                size='xx-small',fontstyle='italic',backgroundcolor='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap and Jackknife\n",
    "\n",
    "code from [ https://github.com/astroML/astroML/blob/master/astroML/resample.py ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "\n",
    "def bootstrap(data, n_bootstraps=10000, user_statistic=lambda x:np.mean(x,axis=1), kwargs=None,\n",
    "              pass_indices=False, random_state=1):\n",
    "    \"\"\"Compute bootstraped statistics of a dataset.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array_like\n",
    "        An n-dimensional data array of size n_samples by n_attributes\n",
    "    n_bootstraps : integer\n",
    "        the number of bootstrap samples to compute.  Note that internally,\n",
    "        two arrays of size (n_bootstraps, n_samples) will be allocated.\n",
    "        For very large numbers of bootstraps, this can cause memory issues.\n",
    "    user_statistic : function\n",
    "        The statistic to be computed.  This should take an array of data\n",
    "        of size (n_bootstraps, n_samples) and return the row-wise statistics\n",
    "        of the data. default: lambda x:np.mean(x,axis=1)\n",
    "    kwargs : dictionary (optional)\n",
    "        A dictionary of keyword arguments to be passed to the\n",
    "        user_statistic function.\n",
    "    pass_indices : boolean (optional)\n",
    "        if True, then the indices of the points rather than the points\n",
    "        themselves are passed to `user_statistic`\n",
    "    random_state: RandomState or an int seed (0 by default)\n",
    "        A random number generator instance\n",
    "    Returns\n",
    "    -------\n",
    "    distribution : ndarray\n",
    "        the bootstrapped distribution of statistics (length = n_bootstraps)\n",
    "    \"\"\"\n",
    "    # we don't set kwargs={} by default in the argument list, because using\n",
    "    # a mutable type as a default argument can lead to strange results\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    rng = check_random_state(random_state)\n",
    "    data = np.asarray(data)\n",
    "    if data.ndim != 1:\n",
    "        n_samples = data.shape[0]\n",
    "        logging.warning(\"bootstrap data are n-dimensional: assuming ordered n_samples by n_attributes\")\n",
    "    else:\n",
    "        n_samples = data.size\n",
    "\n",
    "    # Generate random indices with repetition\n",
    "    ind = rng.randint(n_samples, size=(n_bootstraps, n_samples))\n",
    "    data = data[ind].reshape(-1, data[ind].shape[-1])\n",
    "    # Call the function\n",
    "    if pass_indices:\n",
    "        stat_bootstrap = user_statistic(ind, **kwargs)\n",
    "    else:\n",
    "        stat_bootstrap = user_statistic(data, **kwargs)\n",
    "        \n",
    "    return stat_bootstrap\n",
    "\n",
    "\n",
    "def jackknife(data, user_statistic=lambda x:np.mean(x,axis=1), kwargs=None,\n",
    "              return_raw_distribution=False, pass_indices=False):\n",
    "    \"\"\"Compute first-order jackknife statistics of the data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array_like\n",
    "        A 1-dimensional data array of size n_samples\n",
    "    user_statistic : function\n",
    "        The statistic to be computed.  This should take an array of data\n",
    "        of size (n_samples, n_samples - 1) and return an array of size\n",
    "        n_samples or tuple of arrays of size n_samples, representing the\n",
    "        row-wise statistics of the input. default:lambda x:np.mean(x,axis=1)\n",
    "    kwargs : dictionary (optional)\n",
    "        A dictionary of keyword arguments to be passed to the\n",
    "        user_statistic function.\n",
    "    return_raw_distribution : boolean (optional)\n",
    "        if True, return the raw jackknife distribution.  Be aware that\n",
    "        this distribution is not reflective of the true distribution:\n",
    "        it is simply an intermediate step in the jackknife calculation\n",
    "    pass_indices : boolean (optional)\n",
    "        if True, then the indices of the points rather than the points\n",
    "        themselves are passed to `user_statistic`\n",
    "    Returns\n",
    "    -------\n",
    "    mean, stdev : floats\n",
    "        The mean and standard deviation of the jackknifed distribution\n",
    "    raw_distribution : ndarray\n",
    "        Returned only if `return_raw_distribution` is True\n",
    "        The array containing the raw distribution (length n_samples)\n",
    "        Be aware that this distribution is not reflective of the true\n",
    "        distribution: it is simply an intermediate step in the jackknife\n",
    "        calculation\n",
    "    Notes\n",
    "    -----\n",
    "    This implementation is a leave-one-out jackknife.\n",
    "    Jackknife resampling is known to fail on rank-based statistics\n",
    "    (e.g. median, quartiles, etc.)  It works well on smooth statistics\n",
    "    (e.g. mean, standard deviation, etc.)\n",
    "    \"\"\"\n",
    "    # we don't set kwargs={} by default in the argument list, because using\n",
    "    # a mutable type as a default argument can lead to strange results\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    data = np.asarray(data)\n",
    "    n_samples = data.size\n",
    "\n",
    "    if data.ndim != 1:\n",
    "        raise ValueError(\"bootstrap expects 1-dimensional data\")\n",
    "\n",
    "    # generate indices for the entire dataset, converting to row vector\n",
    "    ind0 = np.arange(n_samples)[np.newaxis, :]\n",
    "\n",
    "    # generate sets of indices where a single datapoint is left-out\n",
    "    ind = np.arange(n_samples, dtype=int)\n",
    "    ind = np.vstack([np.hstack((ind[:i], ind[i + 1:])) for i in ind])\n",
    "\n",
    "    # compute the statistic for the whole dataset\n",
    "    if pass_indices:\n",
    "        stat_data = user_statistic(ind0, **kwargs)\n",
    "        stat_jackknife = user_statistic(ind, **kwargs)\n",
    "    else:\n",
    "        stat_data = user_statistic(data[ind0], **kwargs)\n",
    "        stat_jackknife = user_statistic(data[ind], **kwargs)\n",
    "\n",
    "    # handle multiple statistics:\n",
    "    # if ndim=0, then the statistic is not operating on rows (error).\n",
    "    # if ndim=1, then it's a single statistic returned\n",
    "    # if ndim=2, then a tuple has been returned\n",
    "    stat_data = np.asarray(stat_data)\n",
    "    ndim = stat_data.ndim\n",
    "\n",
    "    if ndim == 0:\n",
    "        raise ValueError(\"user_statistic should return row-wise statistics\")\n",
    "\n",
    "    stat_data = np.atleast_2d(stat_data).T\n",
    "    stat_jackknife = np.atleast_2d(stat_jackknife)\n",
    "\n",
    "    # compute the jackknife correction formula\n",
    "    delta_stat = (n_samples - 1) * (stat_data - stat_jackknife.mean(1))\n",
    "    stat_corrected = (stat_data + delta_stat)[0]\n",
    "    sigma_stat = np.sqrt(1. / n_samples / (n_samples + 1)\n",
    "                         * np.sum((n_samples * stat_data - stat_corrected\n",
    "                                   - (n_samples - 1)\n",
    "                                   * stat_jackknife.T) ** 2, 0))\n",
    "\n",
    "    if return_raw_distribution:\n",
    "        results = tuple(zip(stat_corrected, sigma_stat, stat_jackknife))\n",
    "    else:\n",
    "        results = tuple(zip(stat_corrected, sigma_stat))\n",
    "\n",
    "    if ndim == 1:\n",
    "        return results[0]\n",
    "    else:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample size control by random selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample_size_control:\n",
    "    def __init__(self,func,animalList,NbAnimal,n,**kwargs):\n",
    "        \"\"\"\n",
    "        func: function to be applied to the randomly chosen animals\n",
    "        NbAnimal: number of animals to be considered in each iteration of func\n",
    "        n: max number of iterations\n",
    "        kwargs: give any input necessary to run \"func\" comma-seperated, like normal function arguments\n",
    "        \"\"\"\n",
    "        if NbAnimal>len(animalList):\n",
    "            raise (\"NbAnimal must be smaller than animal list\")\n",
    "#         if n==0:\n",
    "#             tmp=scipy.special.comb(len(animalList), NbAnimal, exact=True, repetition=False)\n",
    "#             n=max([1000,tmp])\n",
    "        \n",
    "        self.iterN=n\n",
    "        self.animalList=animalList\n",
    "        self.func=func\n",
    "        self.subsetSize=NbAnimal\n",
    "        self.kwargs=kwargs\n",
    "        \n",
    "        self.animalRepeat=np.ones(len(self.animalList))\n",
    "\n",
    "        self.Results=self.run_function()\n",
    "        \n",
    "        \n",
    "    def random_animal_subset(self):\n",
    "        prob=np.sum(self.animalRepeat)*(1/self.animalRepeat)\n",
    "        prob=prob/np.sum(prob)\n",
    "        animalListSubset=np.random.choice(a=self.animalList,size=self.subsetSize,replace=False,p=prob)\n",
    "        self.animalRepeat+=[animal in animalListSubset for animal in self.animalList]\n",
    "\n",
    "        return animalListSubset\n",
    "            \n",
    "    def run_function(self):\n",
    "        #inputArgs=inspect.getargspec(self.func)[0]\n",
    "        i=0\n",
    "        result=[]\n",
    "        Args=self.kwargs\n",
    "\n",
    "        #calculating estimated time needed to process\n",
    "        Args.update({'animalList':self.random_animal_subset()})\n",
    "        result.append(self.func(**Args))\n",
    "        Args.update({'animalList':self.random_animal_subset()})\n",
    "        result.append(self.func(**Args))\n",
    "        i=2\n",
    "\n",
    "        while i<self.iterN:\n",
    "            Args.update({'animalList':self.random_animal_subset()})\n",
    "            result.append(self.func(**Args))\n",
    "            i+=1\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory_PDF(data,TimeRes=.5,PosRes=5,onlyGood=False,**kargs):\n",
    "    \"\"\"\n",
    "    calculates and plots the joint PDF of trajectories.\n",
    "    time resolution in seconds\n",
    "    Position resolution in cm\n",
    "    \"\"\"\n",
    "    \n",
    "    #data=Data(root,session[:6],session,defaultParam,redoPreprocess=False)\n",
    "    allTraj=get_positions_array_beginning(data,onlyGood).T\n",
    "    trialDuration=scipy.stats.mode(data.maxTrialDuration)[0]\n",
    "\n",
    "    posSize =len(np.arange(data.treadmillRange[0],data.treadmillRange[1],PosRes))\n",
    "    timeSize=len(np.arange(0,trialDuration,TimeRes))\n",
    "    trajDis=np.zeros([timeSize,posSize])\n",
    "    \n",
    "    #replacing nans w/ the last position\n",
    "    allTraj=allTraj//PosRes\n",
    "\n",
    "    for t in range(allTraj.shape[0]-1):\n",
    "        timeIndex=int((t/data.cameraSamplingRate)//TimeRes)\n",
    "        trajDis[timeIndex,:]=[np.sum(allTraj[t,:]==x) for x in range(posSize)]\n",
    "\n",
    "    trajDis=scipy.ndimage.filters.gaussian_filter(trajDis, sigma=[1,1],\n",
    "                                                  order=0, mode='nearest', truncate=3)\n",
    "    #normalizing as a PDF\n",
    "    trajDis/=np.sum(trajDis)\n",
    "\n",
    "#     plt.figure();\n",
    "    plt.pcolor(trajDis.T, cmap=cm.hot,**kargs);\n",
    "    ax=plt.gca();\n",
    "    ax.set_xticks     (np.linspace(0,timeSize,5));\n",
    "    ax.set_xticklabels(np.linspace(0,trialDuration,5));\n",
    "    ax.set_yticks     (np.linspace(0,posSize,10));\n",
    "    ax.set_yticklabels(np.linspace(data.treadmillRange[0],data.treadmillRange[1],10));\n",
    "    \n",
    "    return trajDis\n",
    "\n",
    "def twoD_entropy(trajDist):\n",
    "    H=0\n",
    "    for i in range(trajDist.shape[0]):\n",
    "        for j in range(trajDist.shape[1]):\n",
    "            try:\n",
    "                H+=trajDist[i,j]*math.log(float(trajDist[i,j]),2)\n",
    "            except:\n",
    "                pass\n",
    "    H=-H\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read session files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(data,paramName,extension=\".behav_param\",exclude=None,valueType=str):\n",
    "    '''\n",
    "    Use to read from .behav_param or .entrancetimes\n",
    "    Look for lines containing \"paramName\" and not containing \"exclude\"\n",
    "    Split them by white spaces \n",
    "    example: \"treadmill speed:     30.00\" becomes [\"treadmill\",\"speed:\",\"30.00\"])\n",
    "    Return a list of their last element, in the specified valueType (in example: \"30.00\")\n",
    "    '''\n",
    "    behav=data.fullPath+extension\n",
    "    if not os.path.exists(behav):\n",
    "        print(\"No file %s\"%behav)\n",
    "        data.hasBehavior=False\n",
    "        return []\n",
    "    result=[]\n",
    "    trials=[0]\n",
    "    with open(behav,\"r\") as f:\n",
    "        for line in f:\n",
    "            if \"Trial #\" in line:\n",
    "                trials.append(int(float(line.split()[-1]))-1)\n",
    "            if paramName in line:\n",
    "                if (exclude is not None) and (exclude in line):\n",
    "                    continue\n",
    "                res=line.split()[-1]\n",
    "                #integer or float: replace comma by dots\n",
    "                if valueType in [int,float]:\n",
    "                    res=res.replace(\",\",\".\")                 \n",
    "                #integer: convert first to float (\"0.00\" -> 0.00 -> 0)\n",
    "                if valueType is int:\n",
    "                    res=int(float(res))\n",
    "                #boolean \"TRUE\" \"FALSE\"\n",
    "                elif valueType is bool:\n",
    "                    res=(res.lower()==\"true\")\n",
    "                else:\n",
    "                    res=valueType(res)\n",
    "                result.append( (trials[-1],res) )\n",
    "    out=[np.nan]*(trials[-1]+1)\n",
    "    for item in result:\n",
    "        out[item[0]]=item[1]\n",
    "    return np.asarray(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_rate(timePoints,minDis=0,maxDis=np.inf):\n",
    "    \"\"\"\n",
    "    timePoints: list of times of occuring of events (in sec)\n",
    "    minDis= minimum distance between events to be considered valid\n",
    "    \"\"\"\n",
    "    tDiff=np.diff(timePoints)\n",
    "    tDiff=tDiff[np.logical_and(tDiff>minDis,tDiff<maxDis)]\n",
    "    return 1/np.nanmean(tDiff)\n",
    "\n",
    "def compute_rate (x,winLen,overlap=0.5,zero=0,end=None):\n",
    "    \"\"\"\n",
    "    x: list like data with times of event, in sec\n",
    "    winLen: length of window in sec\n",
    "    overlap: normalized overlap: (0,1)\n",
    "    zero: begining of the time axis\n",
    "    end: maximum of time axis\n",
    "    ??window: window param of scipy.signal.get_window\n",
    "    \"\"\"\n",
    "    assert overlap<1 and overlap>0, \"bad overlap value\"\n",
    "    x=np.array(x)\n",
    "    if end is None:\n",
    "        end=x[-1]\n",
    "#     if window is None:\n",
    "#         window='boxcar'\n",
    "#     win=scipy.signal.get_window(window,winLen)\n",
    "    Range=np.arange(zero,end,(1-overlap)*winLen)\n",
    "    out=[]\n",
    "    for i,_ in enumerate(Range):\n",
    "        a=x[np.logical_and(x>=Range[i],x<Range[i]+winLen)]\n",
    "        out.append(len(a)/winLen)\n",
    "    return np.array(out),Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy PRB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prb_copy (prbfile, animalFolder):\n",
    "    \"\"\"\n",
    "    prbfile='/home/david/Mostafa/info/prb-config files/8tetrode_8channelgroup.prb'\n",
    "    animalFolder='/NETDATA/Rat172/Experiments/'\n",
    "    \n",
    "    \"\"\"\n",
    "    for dat in find_file(animalFolder, extension=['.dat']):\n",
    "        prb2=copy(prbfile,os.path.dirname(dat))\n",
    "        os.rename(prb2,dat[:-3]+'prb')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a single channel from a _*.dat_ file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ephy_epoch(filename, fs, Nch, wantedCh, t0, t1):\n",
    "    assert filename.endswith(('.dat','.DAT','.Dat')), \"bad file type: Not *.DAT\"\n",
    "    \n",
    "    sampleSize=np.dtype(np.int16).itemsize\n",
    "    systembyte=sys.byteorder\n",
    "    n0=int(t0*fs*Nch*sampleSize)\n",
    "    \n",
    "    signal=[]\n",
    "\n",
    "    with open(filename,'rb') as f:\n",
    "        if t1>t0:\n",
    "            n1=int(t1*fs*Nch*sampleSize)\n",
    "        elif t1==-1:\n",
    "            f.seek(0,2)\n",
    "            n1=f.tell()\n",
    "        else:\n",
    "            raise ValueError(\"t1 must be greater than t0, or -1\")\n",
    "\n",
    "        f.seek( n0 + ((wantedCh-1) *sampleSize))\n",
    "        n=n0\n",
    "        step=(Nch-1)*sampleSize\n",
    "        while n < n1:\n",
    "            data=f.read(sampleSize)\n",
    "            signal.append(int.from_bytes(data,systembyte,signed=True))\n",
    "            n+=step+sampleSize\n",
    "            f.seek(step,1)\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "    return np.array(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging several .dat files together \n",
    "(MUST have the same number of channels and sampling frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dat_merger(files: list, nCh: int):\n",
    "    \"\"\"\n",
    "    files: a list of all the dat file paths you wish to merge (a list of strings)\n",
    "    nCh: number of channels (int)\n",
    "    \"\"\"\n",
    "    dat=[]\n",
    "    for file in files:\n",
    "        data=np.fromfile(file)\n",
    "        data=np.reshape(data,(-1,nCh))\n",
    "        dat.append(data)\n",
    "\n",
    "    out=np.concatenate([array for array in dat],axis=0)\n",
    "    del dat\n",
    "    path=f'{os.path.dirname(files[0])}{os.sep}MERGED.dat'\n",
    "    out.tofile(path)\n",
    "    print(f'saved in {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_fetch(root: str, animal: str, profile: dict, PerfParam: list, NbSession: slice =5):\n",
    "    \"\"\"\n",
    "    returns the data requested by PerfParam\n",
    "    PerfParam: a list of known performance parameters or functions recieving data objest as input\n",
    "    \"\"\"\n",
    "    if not isinstance(PerfParam,list):\n",
    "        PerfParam=[PerfParam]\n",
    "    \n",
    "    perf=[]\n",
    "    func=[]\n",
    "    for item in PerfParam:\n",
    "        if isinstance(item,types.FunctionType):\n",
    "            func.append(item)\n",
    "        elif isinstance(item,str):\n",
    "            perf.append(item)\n",
    "\n",
    "    if not isinstance(NbSession,slice):\n",
    "        if NbSession >0:\n",
    "            NbSession=slice(NbSession)\n",
    "        else:\n",
    "            NbSession=slice(NbSession,None)\n",
    "\n",
    "    sessions=batch_get_session_list(root,[animal],profile=profile)['Sessions'][NbSession]\n",
    "#     assert < len(sessions), \"not enough sessions with this profile\"\n",
    "    \n",
    "    res=dict((param,[]) for param in perf)\n",
    "    res.update((param.__name__,[]) for param in func)\n",
    "    for session in sessions:\n",
    "        data=Data(root,session[:6],session,redoPreprocess=False)\n",
    "        \n",
    "        p1=compute_or_read_stats(data, perf, \n",
    "                                 saveAsPickle=False, redo=False)            \n",
    "        for param in perf:\n",
    "            res[param].append(p1[param])\n",
    "            \n",
    "        for fun in func:\n",
    "            res[fun.__name__].append(fun(data))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Times when trials end (Treadmill stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "def punishment_duration(data,trial,minDuration=1,maxDuration=10):\n",
    "    beamIgnore=read_file(data,paramName=\"consider beam state after (s)\",valueType=float)[trial]\n",
    "    punishTime=maxDuration-maxDuration*(data.entranceTime[trial]-beamIgnore)/(data.goalTime[trial]-beamIgnore)\n",
    "    punishTime=max((punishTime,minDuration))\n",
    "    return punishTime\n",
    "\n",
    "def detect_trial_end(data, trials=None):\n",
    "    if trials is None:\n",
    "        trials=data.trials\n",
    "        \n",
    "    for trial in trials:\n",
    "        if data.timeEndTrial[trial] is not None:\n",
    "            if trial in data.goodTrials or data.timeEndTrial[trial] >= data.entranceTime[trial]+.99:\n",
    "                continue\n",
    "        if data.entranceTime[trial] > data.goalTime[trial]:\n",
    "            data.timeEndTrial[trial]=data.entranceTime[trial]\n",
    "            data.indexEndTrial[trial]=int_(data.timeEndTrial[trial]*data.cameraSamplingRate)\n",
    "            continue\n",
    "        else: #implementing the punishment rule\n",
    "            data.timeEndTrial[trial]=data.entranceTime[trial]+punishment_duration(data,trial)\n",
    "            data.indexEndTrial[trial]=int_(data.timeEndTrial[trial]*data.cameraSamplingRate)\n",
    "            \n",
    "    return data.timeEndTrial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Ordered colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colors(n, colormap='plasma'):\n",
    "    colors = []\n",
    "    cmap = plt.cm.get_cmap(colormap)\n",
    "    for colorVal in np.linspace(0, 1, n+1):\n",
    "        colors.append(cmap(colorVal))\n",
    "    return colors[:-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
